import tensorflow as tf
from tensorflow.python.saved_model import tag_constants
import cv2
import numpy as np
from core.config import cfg
import colorsys
import random
import time

MODEL_PATH = './checkpoints/yolov4-416'
IOU_THRESHOLD = 0.45
SCORE_THRESHOLD = 0.25
INPUT_SIZE = 416

# load model
saved_model_loaded = tf.saved_model.load(MODEL_PATH, tags=[tag_constants.SERVING])
infer = saved_model_loaded.signatures['serving_default']
#tracker
tracker = None
isFirst = True


def read_class_names(class_file_name):
    names = {}
    with open(class_file_name, 'r') as data:
        for ID, name in enumerate(data):
            names[ID] = name.strip('\n')
    return names

def draw_bbox(image, bboxes, classes=read_class_names(cfg.YOLO.CLASSES), show_label=True):
    num_classes = len(classes)
    image_h, image_w, _ = image.shape
    hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]
    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))
    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), 
                      colors))

    random.seed(0)
    random.shuffle(colors)
    random.seed(None)

    out_boxes, out_scores, out_classes, num_boxes = bboxes
    for i in range(num_boxes[0]):
        if int(out_classes[0][i]) < 0 or int(out_classes[0][i]) > num_classes: 
            continue
        coor = out_boxes[0][i]
        coor[0] = int(coor[0] * image_h)
        coor[2] = int(coor[2] * image_h)
        coor[1] = int(coor[1] * image_w)
        coor[3] = int(coor[3] * image_w)

        fontScale = 0.5
        score = out_scores[0][i]
        class_ind = int(out_classes[0][i])
        bbox_color = colors[class_ind]
        bbox_thick = int(0.6 * (image_h + image_w) / 600)
        c1, c2 = (int(coor[1]), int(coor[0])), (int(coor[3]), int(coor[2]))
        cv2.rectangle(image, c1, c2, bbox_color, bbox_thick)

        if show_label:
            bbox_mess = '%s_%d: %.2f' % (classes[class_ind],i, score)
            t_size = cv2.getTextSize(bbox_mess, 0, fontScale, 
																		thickness=bbox_thick // 2)[0]
            c3 = (c1[0] + t_size[0], c1[1] - t_size[1] - 3)
            cv2.rectangle(image, c1, (int(np.float32(c3[0])), int(np.float32(c3[1])))
                          , bbox_color, -1)

            cv2.putText(image, bbox_mess, (c1[0], int(np.float32(c1[1] - 2))), 
                        cv2.FONT_HERSHEY_SIMPLEX,
                        fontScale, (0, 0, 0), bbox_thick // 2, lineType=cv2.LINE_AA)
    return image

def set_roi(img, idx=1):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    img_input = cv2.resize(img, (INPUT_SIZE, INPUT_SIZE))
    img_input = img_input / 255.
    img_input = img_input[np.newaxis, ...].astype(np.float32)
    img_input = tf.constant(img_input)

    pred_bbox = infer(img_input)

    for key, value in pred_bbox.items():
        boxes = value[:, :, 0:4]
        pred_conf = value[:, :, 4:]

    boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(
        boxes=tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4)),
        scores=tf.reshape(
            pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1])),
        max_output_size_per_class=50,
        max_total_size=50,
        iou_threshold=IOU_THRESHOLD,
        score_threshold=SCORE_THRESHOLD
    )

    pred_bbox=[boxes.numpy(), scores.numpy(), classes.numpy(), valid_detections.numpy()]

    classes_of_yolo = read_class_names(cfg.YOLO.CLASSES)
    coor = boxes.numpy()[0][idx]
    image_h, image_w, _ = img.shape
    coor[0] = int(coor[0] * image_h)
    coor[2] = int(coor[2] * image_h)
    coor[1] = int(coor[1] * image_w)
    coor[3] = int(coor[3] * image_w)

    score = scores.numpy()[0][idx]
    class_idx = int(classes.numpy()[0][idx])
    object_name = classes_of_yolo[class_idx]

    return coor, score, object_name ,pred_bbox

def main(video_path):
    isFirst = True
    tracker = None
    win_name = 'YOLOV4 + Tracking APIs'
    cap = cv2.VideoCapture(video_path)
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print('Cannot read video file')
            break
        img_draw = frame.copy()
        cv2.putText(img_draw, 'YOLO v4 + CSRT', (20, 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)
        if tracker: # 트랙커 생성된 경우
            ok, bbox = tracker.update(frame)  # 새로운 프레임에서 추적 위치 찾기 ---③
            (x, y, w, h) = bbox
            if ok:  # 추적 성공
                cv2.rectangle(img_draw, (int(x), int(y)), (int(x + w), int(y + h)),
                              (0, 255, 0), 2, 1)
            else:  # 추적 실패
                cv2.putText(img_draw, "Tracking fail.", (100, 80),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 255), 2, cv2.LINE_AA)
        # trackerName = tracker.__class__.__name__
        if not isFirst:
            bbox_mess = '%s_%d: %.2f' % (ob_name, idx, score)
            image_h, image_w, _ = img_draw.shape
            bbox_thick = int(0.6 * (image_h + image_w) / 600)
            t_size = cv2.getTextSize(bbox_mess, 0, 0.5, thickness=bbox_thick // 2)[0]
            c1 = (int(x), int(y))
            c3 = (c1[0] + t_size[0], c1[1] - t_size[1] - 3)
            cv2.rectangle(img_draw, c1, (int(np.float32(c3[0])), int(np.float32(c3[1]))),
                          (0, 255, 0), -1)

            cv2.putText(img_draw, bbox_mess, (c1[0], int(np.float32(c1[1] - 2))),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        0.5, (0, 0, 0), bbox_thick // 2, lineType=cv2.LINE_AA)
            cv2.imshow(win_name, img_draw)
            key = cv2.waitKey(5) & 0xff

        if isFirst:
            tmp_roi, score, ob_name, pred_bbox = set_roi(img_draw)
            img_draw = draw_bbox(img_draw, pred_bbox)
            cv2.putText(img_draw, "Detected Objects",
                        (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 
												(0, 0, 255), 2, cv2.LINE_AA)
            cv2.putText(img_draw, "Choose the idx",
                        (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, 
												(255, 255, 255), 2, cv2.LINE_AA)
            cv2.putText(img_draw, 'YOLO v4 + CSRT', (20, 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, cv2.LINE_AA)
            cv2.imshow(win_name, img_draw)
            key = cv2.waitKey(0) & 0xff

        # 스페이스 바 또는 비디오 파일 최초 실행 ---④

        if key in range(48, 56) and isFirst:
            isFirst = False
            idx = key-48
            tmp_roi, score, ob_name, pred_bbox = set_roi(img_draw,idx)
            (y,x,y_h,x_w) = tmp_roi
            roi = (x,y,x_w-x,y_h-y)
            if roi[2] and roi[3]:  # 위치 설정 값 있는 경우
                tracker = cv2.legacy.TrackerCSRT_create()  # 트랙커 객체 생성 ---⑤
                isInit = tracker.init(frame, roi)
        elif key == ord('q'):
            break

        # result = cv2.cvtColor(np.array(result), cv2.COLOR_RGB2BGR)



if __name__ == '__main__':
    video_path = './data/highway.mp4'
    main(video_path)
